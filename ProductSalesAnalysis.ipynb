import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from math import sqrt
%matplotlib inline
from scipy import stats, linalg
from pandas import Series
from sklearn import preprocessing
from statsmodels.tsa.arima_model import ARIMA
from pandas.tools.plotting import autocorrelation_plot
from statsmodels.graphics.tsaplots import plot_acf
import itertools

df = pd.read_csv(r"C:\Users\vx15\Downloads\Sales_Transactions_Dataset_Weekly.csv")

#select the columns with the normalized sales and use the product code as the index
normalized_cols = [col for col in df.columns if ('Normalize' in col or 'Product_Code' in col)]

#function to create a new dataframe with the select columns
def select_columns(data_frame, column_names):
    new_frame = data_frame.loc[:, column_names]
    return new_frame

#use function to extract normalized columns
normalized_df = select_columns(df,normalized_cols)


#Set index to product code to transpose the matrix properly
normalized_df.set_index('Product_Code',inplace=True)


#transpose the normalized matrix to compare product sales
normalized_dft = normalized_df.transpose()

#find the products' correlation matrix
corrMatrix = normalized_dft.corr()

#find the covariance matrix for the given dataframe
covarMatrix = normalized_dft.cov()
#print(covarMatrix)

#print(p1_analysis_df.head())

#create an array with 52 weekly dates to fit ARIMA model with
ts = pd.DataFrame()
ts['Date'] = pd.date_range('1/1/2018', periods = 52, freq ='W')


#Add product 1 sales to time series dataframe
ts_sales = df.T.iloc[55:,0]
ts['Normalized P1 Sales'] = ts_sales.reset_index(drop=True)


#set time series index to 'date'
ts.set_index('Date',inplace=True)


#ts_diff.plot()
#plt.ylabel('Normalized P1 Sales')
#plt.grid()



#Use the first 44 values in the time series as training values and the last 8 as test values for ARIMA
X = ts.values
train = X[0:44]
test = X[44:]


#Function to automatically find best pdq parameters with relation to the minimum AIC value
def findBestParam(train):
    import warnings
    warnings.filterwarnings('ignore')
    p=d=q=range(0,5)
    pdq = list(itertools.product(p,d,q))
    dictionary = {}
    for param in pdq:
        try:
            model_arima = ARIMA(train,order = param)
            model_arima_fit = model_arima.fit()
            dictionary[model_arima_fit.aic] = param
        except:
            continue
            
    return min(dictionary.items(), key=lambda x: x[1])


#Fit and forecast the test values using the ARIMA model with the best parameters found above
model_arima = ARIMA(train,(0,0,0))
model_arima_fit = model_arima.fit()
predictions = model_arima_fit.forecast(steps=8)[0]


#plt.plot(test)
#plt.plot(predictions,color='green')




#Clustering using affinity propagation


from sklearn.cluster import KMeans
from sklearn.cluster import AffinityPropagation
from sklearn import metrics
from sklearn.datasets.samples_generator import make_blobs
from itertools import cycle
from sklearn import cluster

#All time series, with the weeks being used as features, so that i can cluster them
normalized_ts_cluster = df.iloc[:,55:]

X, labels_true = make_blobs(n_samples=300, centers=normalized_ts_cluster, cluster_std=0.5, random_state=0)
# Compute Affinity Propagation
af = AffinityPropagation(preference=-50).fit(X)
cluster_centers_indices = af.cluster_centers_indices_
labels = af.labels_

n_clusters_ = len(cluster_centers_indices)

#Assign a color to each cluster, then zip through, each tuple contains the number of the cluster and the respective color
colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
for k, col in zip(range(n_clusters_), colors):
    class_members = labels == k
    cluster_center = X[cluster_centers_indices[k]]
    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')
    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
             markeredgecolor='k', markersize=14)
    for x in X[class_members]:
        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)

plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()

#clustering = KMeans(n_clusters=10,random_state=5)
#clustering.fit(normalized_ts_cluster)
#plt.subplot(1,2,1)
#plt.scatter()



